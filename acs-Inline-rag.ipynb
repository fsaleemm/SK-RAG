{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement RAG Pattern using Semantic Kernel and Azure Cognitive Search\n",
    "**NOTE**: This notebook requires that a search index exists with semantic search and vector index enabled.\n",
    "\n",
    "Follow the steps in the [Notebook](https://github.com/fsaleemm/cognitive-search-vector-pr/blob/main/demo-python/code/azure-search-vector-python-sample.ipynb) to create the vector index with semantic search enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install python-dotenv==1.0.0\n",
    "!python -m pip install --upgrade semantic-kernel\n",
    "!python -m pip install azure-search-documents==11.4.0b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "if not load_dotenv(): raise Exception(\".env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x1d1412a8410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "kernel.add_text_embedding_generation_service(\n",
    "    \"ada\",\n",
    "    AzureTextEmbedding(\n",
    "        os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "kernel.add_chat_service(\n",
    "    \"chat\",\n",
    "    AzureChatCompletion(\n",
    "        os.getenv(\"AZURE_OPENAI_CHAT_MODEL\"),\n",
    "        os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the intent of the question being asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What services are best for asynchornous communications?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inquiring about asynchronous communication services\n"
     ]
    }
   ],
   "source": [
    "#intent detection\n",
    "\n",
    "from semantic_kernel import PromptTemplate, PromptTemplateConfig, SemanticFunctionConfig\n",
    "\n",
    "\n",
    "prompt = \"\"\"Bot: How can I help you?\n",
    "User: {{$input}}\n",
    "\n",
    "---------------------------------------------\n",
    "\n",
    "The intent of the user in 5 words or less: \"\"\"\n",
    "\n",
    "prompt_config = PromptTemplateConfig(\n",
    "    description=\"Gets the intent of the user.\",\n",
    "    type=\"completion\",\n",
    "    completion=PromptTemplateConfig.CompletionConfig(0.0, 0.0, 0.0, 0.0, 500),\n",
    "    input=PromptTemplateConfig.InputConfig(\n",
    "        parameters=[\n",
    "            PromptTemplateConfig.InputParameter(\n",
    "                name=\"input\", description=\"The user's request.\", default_value=\"\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SemanticFunctionConfig object\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt,\n",
    "    template_engine=kernel.prompt_template_engine,\n",
    "    prompt_config=prompt_config,\n",
    ")\n",
    "\n",
    "function_config = SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "\n",
    "get_intent = kernel.register_semantic_function(\n",
    "    skill_name=\"OrchestratorPlugin\",\n",
    "    function_name=\"GetIntent\",\n",
    "    function_config=function_config,\n",
    ")\n",
    "\n",
    "result_intent = await kernel.run_async(\n",
    "    get_intent,\n",
    "    input_str=question,\n",
    ")\n",
    "\n",
    "print(result_intent.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relevant data from the ACS index using hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import openai \n",
    "\n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=\"text-embedding-ada-002\")\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Service Bus is a fully managed, enterprise-grade messaging service that enables you to build reliable and scalable applications. It provides features like message queuing, publish-subscribe, and dead-lettering. Service Bus supports various messaging patterns, including point-to-point, broadcast, and request-reply. You can use Service Bus to integrate your applications and services, decouple your system components, and handle asynchronous communication. It also integrates with other Azure services, such as Azure Functions and Azure Logic Apps.\n",
      "Azure Queue Storage is a fully managed, scalable, and durable message queuing service that enables you to decouple your applications and build asynchronous solutions. It provides features like at-least-once delivery, message time-to-live, and a RESTful API. Queue Storage supports various programming languages, such as C#, Java, and Python. You can use Azure Queue Storage to build distributed applications, offload your databases, and process and store large volumes of messages. It also integrates with other Azure services, such as Azure Functions and Azure Logic Apps.\n",
      "Azure SignalR Service is a fully managed, real-time messaging service that enables you to build and scale real-time web applications. It provides features like automatic scaling, WebSocket support, and serverless integration. SignalR Service supports various programming languages, such as C#, JavaScript, and Java. You can use Azure SignalR Service to build chat applications, real-time dashboards, and collaborative tools. It also integrates with other Azure services, such as Azure Functions and Azure App Service.\n"
     ]
    }
   ],
   "source": [
    "#get relevant data for context\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient, IndexDocumentsBatch  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector \n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")  \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")  \n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "vector = Vector(value=generate_embeddings(result_intent.result), \n",
    "                k=3, \n",
    "                fields=\"contentVector\" # These may need to be adjusted based on your index configuration\n",
    "            )\n",
    "\n",
    "results_context = search_client.search(  \n",
    "    search_text=result_intent, \n",
    "    vectors= [vector],\n",
    "    select=[\"title\", \"content\", \"category\"], # These may need to be adjusted based on your index configuration\n",
    "    query_type=QueryType.SEMANTIC, \n",
    "    query_language=\"en-us\", \n",
    "    semantic_configuration_name='my-semantic-config', # These may need to be adjusted based on your index configuration\n",
    "    query_caption=\"extractive\", \n",
    "    query_answer=\"extractive|count-2\",\n",
    "    top=3\n",
    ")  \n",
    "\n",
    "results_content = []\n",
    "\n",
    "for r in results_context:\n",
    "    print(r[\"content\"])\n",
    "    results_content.append(r[\"content\"])\n",
    "\n",
    "results =\" \".join(results_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What services are best for asynchornous communications?\n",
      "Optimized Question: Inquiring about asynchronous communication services\n",
      "---\n",
      "Answer: Azure Service Bus and Azure Queue Storage are best for asynchronous communications.\n",
      "\n",
      "Explanation: Azure Service Bus provides features like message queuing and publish-subscribe, which are essential for asynchronous communication. Azure Queue Storage is a message queuing service that enables you to build asynchronous solutions.\n"
     ]
    }
   ],
   "source": [
    "#Summarize\n",
    "\n",
    "import semantic_kernel\n",
    "\n",
    "\n",
    "sprompt = \"\"\"\n",
    "Considering these facts\n",
    "\n",
    "Facts: {{$results}}\n",
    "\n",
    "Question: {{$input}}\n",
    "\n",
    "Provide a concise answer ('Answer: ') and a separate explanation ('Explanation: '), in two lines.\n",
    "\"\"\"\n",
    "\n",
    "sprompt_config = PromptTemplateConfig(\n",
    "    description=\"Gets the intent of the user.\",\n",
    "    type=\"completion\",\n",
    "    completion=PromptTemplateConfig.CompletionConfig(0.5, 0.0, 0.0, 0.0, 1024),\n",
    "    input=PromptTemplateConfig.InputConfig(\n",
    "        parameters=[\n",
    "            PromptTemplateConfig.InputParameter(\n",
    "                name=\"input\", description=\"The user's request.\", default_value=\"\"\n",
    "            ),\n",
    "            PromptTemplateConfig.InputParameter(\n",
    "                name=\"results\", description=\"The result from grounding data\", default_value=\"\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SemanticFunctionConfig object\n",
    "sprompt_template = PromptTemplate(\n",
    "    template=sprompt,\n",
    "    template_engine=kernel.prompt_template_engine,\n",
    "    prompt_config=sprompt_config,\n",
    ")\n",
    "\n",
    "sfunction_config = SemanticFunctionConfig(sprompt_config, sprompt_template)\n",
    "\n",
    "get_summary = kernel.register_semantic_function(\n",
    "    skill_name=\"OrchestratorPlugin\",\n",
    "    function_name=\"GetSummary\",\n",
    "    function_config=sfunction_config,\n",
    ")\n",
    "\n",
    "variables = semantic_kernel.ContextVariables()\n",
    "variables[\"input\"] = question\n",
    "variables[\"results\"] = results\n",
    "\n",
    "result_summary = await kernel.run_async(\n",
    "    get_summary,\n",
    "    input_vars=variables\n",
    ")\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Optimized Question: {result_intent}\")\n",
    "print(\"---\")\n",
    "print(result_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
